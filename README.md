# ğŸš€ ContentAutomation  
## AI-Powered Article Enhancement System

---

## ğŸ“Œ Overview

ContentAutomation is a **full-stack, AI-driven content processing pipeline** designed to automatically **fetch, analyze, and enhance articles at scale**.

The project simulates a **real-world SaaS content automation system** where:

- Raw articles are ingested
- External sources are analyzed
- AI models enhance content
- Results are served via APIs to a frontend dashboard

---

## ğŸ¯ What This System Demonstrates

- Backend API design
- Asynchronous background processing
- AI / LLM integration
- Clean data flow between services
- Production-style system architecture

---

## ğŸ§± System Components

ContentAutomation 



â”œâ”€â”€ Backend (Laravel API)



â”œâ”€â”€ Automation Worker (Node.js)




â”œâ”€â”€ Frontend (Vite + React)




â””â”€â”€ PostgreSQL Database



---

## ğŸ› ï¸ Tech Stack

### ğŸ”¹ Backend (API Layer)

| Technology | Purpose |
|----------|--------|
| Laravel (PHP 8.4) | REST APIs & business logic |
| PostgreSQL | Persistent data storage |
| Eloquent ORM | Database abstraction |
| REST APIs | Service communication |

---

### ğŸ”¹ Automation Worker (Processing Engine)

| Technology | Purpose |
|----------|--------|
| Node.js | Async background jobs |
| Google / Serper API | Search results |
| Web Scraping | Reference extraction |
| Groq / Gemini | AI content generation |
| Retry + Fallback | Fault tolerance |

---

### ğŸ”¹ Frontend (Dashboard)

| Technology | Purpose |
|----------|--------|
| Vite + React | UI rendering |
| Axios | API communication |
| Env config | Deployment ready |

---

## âš™ï¸ Local Setup Instructions

---

### 1ï¸âƒ£ Prerequisites

Ensure the following are installed:

- PHP â‰¥ 8.2
- Composer
- Node.js â‰¥ 18
- PostgreSQL (Local or Neon)
- Git

---

### 2ï¸âƒ£ Clone Repository

git clone https://github.com/<your-username>/ContentAutomation.git
cd ContentAutomation



---

## ğŸ”§ Backend (Laravel) Setup

cd backend-laravel



cp .env.example .env



composer install



php artisan key:generate



### Configure `.env`

DB_CONNECTION=pgsql
DB_HOST=localhost
DB_PORT=5432
DB_DATABASE=content_automation
DB_USERNAME=postgres
DB_PASSWORD=your_password

php artisan migrate
php artisan serve


**Backend URL**
http://127.0.0.1:8000



---

## ğŸ¤– Node Worker Setup (Automation Engine)

The Node Worker **automatically processes the oldest unprocessed articles**.

cd ../node-worker



cp .env.example .env



npm install



### Configure `.env`

API_BASE_URL=http://127.0.0.1:8000/api
GROQ_API_KEY=your_api_key
SERPER_API_KEY=your_api_key

npm start



### âœ… Worker Responsibilities

| Step | Action |
|----|-------|
| 1 | Fetch oldest unprocessed article |
| 2 | Search competitor content |
| 3 | Scrape reference articles |
| 4 | Rewrite content using AI |
| 5 | Update backend via API |

---

## ğŸ¨ Frontend Setup

cd ../frontend
npm install



Create `.env`:

VITE_API_URL=http://127.0.0.1:8000/api

npm run dev



**Frontend URL**
http://localhost:5173



---

## ğŸ”„ Architecture & Data Flow

### ğŸ”¹ High-Level Vertical Architecture

Frontend (React)
â†“
Laravel API
â†“
PostgreSQL Database
â†‘
Node Worker
â†“
Search / Scraper / AI Models



---

### ğŸ” Detailed Execution Flow

Article (Database)



â†“



Node Worker (Scheduler + Orchestrator)



â†“



Search Engine (Serper / Google)



â†“



Web Scraper



â†“



LLM (Groq â†’ Gemini Fallback)



â†“



Laravel API



â†“



Database Update



â†“



Frontend Display



---

## ğŸ§  How Articles Are Picked Automatically

### Backend Role

- Stores articles with processing status
- Exposes APIs:
  - `GET /api/articles`

### Example Article Structure

{
"id": 5,
"title": "...",
"original_content": "...",
"updated_content": null,
"status": "ORIGINAL"
}



### Worker Logic

- Fetch **oldest article** with status `ORIGINAL`
- Process asynchronously
- Update via API
- **No direct DB access**

---

## ğŸ” What Serper Does

| Feature | Benefit |
|------|--------|
| Google Search API | Structured results |
| JSON output | No HTML parsing |
| Faster | Reliable |
| Safe | No scraping Google |

---

## ğŸ•·ï¸ Scraping Phase â€“ Defensive Design

const content = await scrapeContent(url);



### Key Design Rules

- Scraper failures are **non-fatal**
- Blocked (403) pages are skipped
- Partial data is acceptable

â¡ï¸ Ensures **worker never crashes**

---

## ğŸ§  LLM Phase â€“ Groq Model Fallback Strategy

### Production-Grade Fallback Chain

| Priority | Model Size | Reason |
|--------|-----------|--------|
| 1 | Small | Fast & cheap |
| 2 | Medium | Better quality |
| 3 | Large | Fallback only |

âœ” No manual intervention  
âœ” Cost controlled  
âœ” Rate-limit safe  

---

## ğŸ” Why Worker Uses API (Not Direct DB)

### Laravel Responsibilities

- Validate input
- Update database
- Maintain relations
- Serve frontend APIs

### Worker Responsibilities

- Orchestrate automation
- Call external services
- Remain stateless

---

## ğŸ“ˆ Scalability & Future Enhancements

| Scenario | Solution |
|-------|---------|
| Increased load | Multiple workers |
| Heavy jobs | BullMQ / RabbitMQ |
| Scheduling | Cron triggers |
| AI limits | Swap providers |
| Performance | Caching layer |

---

## âš ï¸ Failure Handling Matrix

| Failure | Handling |
|-------|---------|
| Search API fails | Skip article |
| Scraper blocked | Skip URL |
| LLM rate-limited | Fallback model |
| LLM failure | Revert status |
| Worker crash | Restart |
| Backend down | Retry |

---

## ğŸ“ Notes for Reviewers

- Worker intentionally separated (real SaaS pattern)
- No direct DB access from worker
- Horizontally scalable architecture
- Designed for clarity & extensibility

---

## âœ… Project Status

| Component | Status |
|--------|--------|
| Backend API | âœ” Working |
| Database | âœ” Connected |
| Worker | âœ” Processing |
| Frontend | âœ” Integrated |

---
